{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8519ac43-da2f-4e72-8c8f-63d200c8e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 0.6841, Validation Loss: 0.6646\n",
      "Epoch 2/30, Training Loss: 0.6296, Validation Loss: 0.5707\n",
      "Epoch 3/30, Training Loss: 0.5016, Validation Loss: 0.3976\n",
      "Epoch 4/30, Training Loss: 0.2961, Validation Loss: 0.3564\n",
      "Epoch 5/30, Training Loss: 0.2665, Validation Loss: 0.2298\n",
      "Epoch 6/30, Training Loss: 0.1928, Validation Loss: 0.2154\n",
      "Epoch 7/30, Training Loss: 0.1622, Validation Loss: 0.1847\n",
      "Epoch 8/30, Training Loss: 0.1234, Validation Loss: 0.1904\n",
      "Epoch 9/30, Training Loss: 0.1072, Validation Loss: 0.2267\n",
      "Epoch 10/30, Training Loss: 0.0889, Validation Loss: 0.2103\n",
      "Epoch 11/30, Training Loss: 0.0772, Validation Loss: 0.1674\n",
      "Epoch 12/30, Training Loss: 0.0898, Validation Loss: 0.1648\n",
      "Epoch 13/30, Training Loss: 0.1043, Validation Loss: 0.3593\n",
      "Epoch 14/30, Training Loss: 0.1142, Validation Loss: 0.3455\n",
      "Epoch 15/30, Training Loss: 0.0997, Validation Loss: 0.3371\n",
      "Epoch 16/30, Training Loss: 0.1111, Validation Loss: 0.2246\n",
      "Epoch 17/30, Training Loss: 0.0741, Validation Loss: 0.1452\n",
      "Epoch 18/30, Training Loss: 0.0597, Validation Loss: 0.1481\n",
      "Epoch 19/30, Training Loss: 0.0636, Validation Loss: 0.1489\n",
      "Epoch 20/30, Training Loss: 0.0602, Validation Loss: 0.2157\n",
      "Epoch 21/30, Training Loss: 0.0385, Validation Loss: 0.4479\n",
      "Epoch 22/30, Training Loss: 0.0579, Validation Loss: 0.1888\n",
      "Early stopping triggered\n",
      "模型已保存到 best_combined_model.pth\n",
      "Accuracy: 0.9576\n",
      "AUC: 0.9736\n",
      "Specificity: 0.9153\n",
      "Sensitivity: 1.0000\n",
      "MCC: 0.9186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/9b87nkc953d0wrjp2kz63r2h0000gn/T/ipykernel_27249/2520542316.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_combined_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(630)\n",
    "\n",
    "# Load the CSV files into pandas DataFrame\n",
    "train_320 = pd.read_csv('Cas300_esm.csv')\n",
    "test_320 = pd.read_csv('Cas118_esm.csv')\n",
    "\n",
    "train_aatp = pd.read_csv('train_aatp.csv')\n",
    "test_aatp = pd.read_csv('test_aatp.csv')\n",
    "\n",
    "# Convert pandas DataFrame to torch tensors\n",
    "train_320 = torch.tensor(train_320.values).float()\n",
    "test_320 = torch.tensor(test_320.values).float()\n",
    "train_aatp = torch.tensor(train_aatp.values).float()\n",
    "test_aatp = torch.tensor(test_aatp.values).float()\n",
    "\n",
    "# Load the label files\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "# Prepare data for DataLoader\n",
    "train_dataset = TensorDataset(train_320.unsqueeze(1), train_aatp, torch.tensor(y_train).long())\n",
    "test_dataset = TensorDataset(test_320.unsqueeze(1), test_aatp, torch.tensor(y_test).long())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, conv_window_sizes=[9, 11, 13], aatp_input_size=420):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        \n",
    "        # Convolutional part for 320-dimensional features\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=1, out_channels=4, kernel_size=w, padding=w//2) for w in conv_window_sizes\n",
    "        ])\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        conv_output_size = 4 * len(conv_window_sizes) * (320 // 2)\n",
    "        \n",
    "        # Fully connected layer to reduce the dimension of CNN output\n",
    "        self.fc_reduce = nn.Linear(conv_output_size, 256)  # Reduce to 256\n",
    "        \n",
    "        # Fully connected layers for 400-dimensional AATP features\n",
    "        self.fc_aatp1 = nn.Linear(aatp_input_size, 128)\n",
    "        self.fc_aatp2 = nn.Linear(128, 64)\n",
    "        \n",
    "        # Final fully connected layers after concatenation\n",
    "        self.fc1 = nn.Linear(256 + 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x_320, x_aatp):\n",
    "        # Convolutional processing of 320-dimensional input\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = self.pool(self.relu(conv(x_320)))\n",
    "            conv_outputs.append(conv_out.view(conv_out.size(0), -1))\n",
    "        x_conv = torch.cat(conv_outputs, dim=1)\n",
    "        \n",
    "        # Reduce the dimension of CNN output\n",
    "        x_conv = self.relu(self.fc_reduce(x_conv))  # Shape: (batch_size, 256)\n",
    "        \n",
    "        # Fully connected processing of 400-dimensional AATP input\n",
    "        x_aatp = self.relu(self.fc_aatp1(x_aatp))\n",
    "        x_aatp = self.relu(self.fc_aatp2(x_aatp))  # Shape: (batch_size, 64)\n",
    "        \n",
    "        # Concatenation of reduced CNN output and fully connected outputs\n",
    "        x = torch.cat((x_conv, x_aatp), dim=1)  # Shape: (batch_size, 256 + 64)\n",
    "        \n",
    "        # Final classification layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Early Stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_model = model.state_dict()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Training function with early stopping and saving the best model\n",
    "def train_model_with_early_stopping(model, train_loader, test_loader, criterion, optimizer, epochs=20, patience=5, model_save_path=\"best_model.pth\"):\n",
    "    early_stopping = EarlyStopping(patience=patience, delta=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x_320, x_aatp, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_320, x_aatp)\n",
    "            loss = criterion(outputs, labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validate the model on the test set\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_320, x_aatp, labels in test_loader:\n",
    "                outputs = model(x_320, x_aatp)\n",
    "                loss = criterion(outputs, labels.view(-1))\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(test_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Training Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Check if early stopping is required\n",
    "        early_stopping(val_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            model.load_state_dict(early_stopping.best_model)  # Load the best model\n",
    "            break\n",
    "\n",
    "    # Save the best model to the specified path\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"模型已保存到 {model_save_path}\")\n",
    "\n",
    "# Instantiate the combined model\n",
    "model = CombinedModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model with early stopping and save the best model\n",
    "train_model_with_early_stopping(\n",
    "    model, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    epochs=30, \n",
    "    patience=5, \n",
    "    model_save_path=\"best_combined_model.pth\"\n",
    ")\n",
    "\n",
    "# Evaluation function with corrected AUC calculation\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []  # 用于保存模型输出的概率\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_320, x_aatp, labels in test_loader:\n",
    "            outputs = model(x_320, x_aatp)\n",
    "            probs = F.softmax(outputs, dim=1)[:, 1]  # 获取正类的概率\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.view(-1)).sum().item()\n",
    "            \n",
    "            all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())  # 保存概率\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)  # 使用概率计算AUC\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    sp = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    sn = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    \n",
    "    return acc, auc, sp, sn, mcc\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load(\"best_combined_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "acc, auc, sp, sn, mcc = evaluate_model(model, test_loader)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Accuracy: {acc:.4f}')\n",
    "print(f'AUC: {auc:.4f}')\n",
    "print(f'Specificity: {sp:.4f}')\n",
    "print(f'Sensitivity: {sn:.4f}')\n",
    "print(f'MCC: {mcc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv1)",
   "language": "python",
   "name": "jupyterlab_venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
